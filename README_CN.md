<p align="left">
   <a href="README.md">English</a>  ï½œ ä¸­æ–‡</a>&nbsp
</p>
<br><br>

<p align="center">
 <img src="https://dscache.tencent-cloud.cn/upload/uploader/hunyuan-64b418fd052c033b228e04bc77bbc4b54fd7f5bc.png" width="400"/> <br>
</p><p></p>


<p align="center">
    ğŸ¤—&nbsp;<a href="https://huggingface.co/tencent/"><b>Hugging Face</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;
    <img src="https://avatars.githubusercontent.com/u/109945100?s=200&v=4" width="16"/>&nbsp;<a href="https://modelscope.cn/models/Tencent-Hunyuan/"><b>ModelScope</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;
<img src="https://cdn-avatars.huggingface.co/v1/production/uploads/6594d0c6c5f1cd69a48b261d/04ZNQlAfs08Bfg4B1o3XO.png" width="14"/>&nbsp;<a href="https://github.com/Tencent/AngelSlim/tree/main"><b>AngelSlim</b></a>
</p>

<p align="center">
    ğŸ–¥ï¸&nbsp;<a href="https://hunyuan.tencent.com" style="color: red;"><b>Official Website</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;
    ğŸ•–&nbsp;<a href="https://cloud.tencent.com/product/hunyuan"><b>HunyuanAPI</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;
    ğŸ•¹ï¸&nbsp;<a href="https://hunyuan.tencent.com/"><b>Demo</b></a>&nbsp;&nbsp;&nbsp;&nbsp;
</p>

<p align="center">
    <a href="https://github.com/Tencent-Hunyuan/Hunyuan-7B"><b>GITHUB</b></a> |
    <a href="https://cnb.cool/tencent/hunyuan/Hunyuan-7B"><b>cnb.cool</b></a> |
    <a href="https://github.com/Tencent-Hunyuan/Hunyuan-7B/blob/main/LICENSE"><b>LICENSE</b></a>
</p>




## æ¨¡å‹ä»‹ç»

æ··å…ƒæ˜¯è…¾è®¯å¼€æºçš„é«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼Œä¸“ä¸ºå¤šæ ·åŒ–è®¡ç®—ç¯å¢ƒä¸­çš„çµæ´»éƒ¨ç½²è€Œè®¾è®¡ã€‚ä»è¾¹ç¼˜è®¾å¤‡åˆ°é«˜å¹¶å‘ç”Ÿäº§ç³»ç»Ÿï¼Œè¿™äº›æ¨¡å‹å‡­å€Ÿå…ˆè¿›çš„é‡åŒ–æ”¯æŒå’Œè¶…é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼Œåœ¨å„ç§åœºæ™¯ä¸‹éƒ½èƒ½æä¾›æœ€ä¼˜æ€§èƒ½ã€‚

æˆ‘ä»¬å‘å¸ƒäº†ä¸€ç³»åˆ—æ··å…ƒç¨ å¯†æ¨¡å‹ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒä¸¤ç§å˜ä½“ï¼Œå‚æ•°è§„æ¨¡æ¶µç›–0.5Bã€1.8Bã€4Bå’Œ7Bã€‚è¿™äº›æ¨¡å‹é‡‡ç”¨äº†ä¸æ··å…ƒ-A13Bç›¸ä¼¼çš„è®­ç»ƒç­–ç•¥ï¼Œå› æ­¤ç»§æ‰¿äº†å…¶å¼ºå¤§çš„æ€§èƒ½ç‰¹å¾ã€‚è¿™ä¸ªå…¨é¢çš„æ¨¡å‹å®¶æ—æ”¯æŒçµæ´»çš„éƒ¨ç½²ä¼˜åŒ– - ä»ä½¿ç”¨å°å°ºå¯¸çš„æ¨¡å‹é€‚é…èµ„æºå—é™è¾¹ç¼˜è®¡ç®—åœºæ™¯ï¼Œåˆ°ä½¿ç”¨è¾ƒå¤§å°ºå¯¸çš„é«˜æ€§èƒ½æ¨¡å‹æ”¯æŒé«˜å¹¶å‘ä½å»¶è¿Ÿçš„å¤æ‚æ¨ç†ç”Ÿäº§ç¯å¢ƒï¼Œåœ¨å„ç§åœºæ™¯ä¸‹éƒ½èƒ½ä¿æŒå¼ºå¤§çš„èƒ½åŠ›ã€‚


### æ ¸å¿ƒç‰¹æ€§ä¸ä¼˜åŠ¿
- â€‹**æ··åˆæ¨ç†æ”¯æŒ**â€‹ï¼šåŒæ—¶æ”¯æŒå¿«æ€è€ƒå’Œæ…¢æ€è€ƒä¸¤ç§æ¨¡å¼ï¼Œæ”¯æŒç”¨æˆ·çµæ´»é€‰æ‹©
- â€‹**è¶…é•¿ä¸Šä¸‹æ–‡ç†è§£**â€‹ï¼šåŸç”Ÿæ”¯æŒ256Kä¸Šä¸‹æ–‡çª—å£ï¼Œåœ¨é•¿æ–‡æœ¬ä»»åŠ¡ä¸­ä¿æŒç¨³å®šæ€§èƒ½
- â€‹**å¢å¼ºAgentèƒ½åŠ›**â€‹ï¼šä¼˜åŒ–Agentèƒ½åŠ›ï¼Œåœ¨BFCL-v3ã€Ï„-Benchã€C3-Benchç­‰æ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•ä¸­é¢†å…ˆ
- â€‹**é«˜æ•ˆæ¨ç†**â€‹ï¼šé‡‡ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ç­–ç•¥ï¼Œæ”¯æŒå¤šé‡åŒ–æ ¼å¼ï¼Œå®ç°é«˜æ•ˆæ¨ç†

## æ–°é—»
<br>

* 2025.7.30 æˆ‘ä»¬åœ¨Hugging Faceå¼€æºäº† **Hunyuan-0.5B-Pretrain** , **Hunyuan-1.8B-Pretrain** , **Hunyuan-4B-Pretrain** , **Hunyuan-7B-Pretrain** , **Hunyuan-0.5B-Instruct** , **Hunyuan-1.8B-Instruct** , **Hunyuan-4B-Instruct** , **Hunyuan-7B-Instruct**ã€‚

## Benchmarkè¯„ä¼°æ¦œå•
| Model            | Hunyuan-0.5B-Pretrain | Hunyuan-1.8B-Pretrain | Hunyuan-4B-Pretrain | Hunyuan-7B-Pretrain|
|:------------------:|:---------------:|:--------------:|:-------------:|:---------------:|
| MMLU             | 54.02          | 64.62         | 74.01        | 79.82         |
| MMLU-Redux              |  54.72         | 64.42        | 73.53       | 79         |
| MMLU-Pro        | 31.15             | 38.65            | 51.91        | 57.79          |
| SuperGPQA    |  17.23         | 24.98          | 27.28           | 30.47          |
| BBH       | 45.92          | 74.32         | 75.17        | 82.95          |
| GPQA             | 27.76             | 35.81            | 43.52        | 44.07          |
| GSM8K | 55.64             | 77.26            | 87.49       | 88.25         |
| MATH             | 42.95          | 62.85          | 72.25        | 74.85          |
| EvalPlus             | 39.71          | 60.67          | 67.76        | 66.96          |
| MultiPL-E            | 21.83          | 45.92         | 59.87        | 60.41          |
| MBPP            | 43.38          | 66.14         | 76.46        | 76.19          |
| CRUX-O         | 30.75             | 36.88           | 56.5        | 60.75          |
| Chinese SimpleQA            | 12.51             | 22.31            | 30.53        | 38.86          |
| simpleQA (5shot)            | 2.38             | 3.61            | 4.21        | 5.69          |


| Topic               |                        Bench                         | Hunyuan-0.5B-Instruct | Hunyuan-1.8B-Instruct | Hunyuan-4B-Instruct | Hunyuan-7B-Instruct|
|:-------------------:|:----------------------------------------------------:|:-------------:|:------------:|:-----------:|:---------------------:|
| **Mathematics**     |            AIME 2024<br>AIME 2025<br>MATH            | 17.2<br>20<br>48.5 | 56.7<br>53.9<br>86 | 78.3<br>66.5<br>92.6 | 81.1<br>75.3<br>93.7 |
| **Science**         |            GPQA-Diamond<br>OlympiadBench             | 23.3<br>29.6 | 47.2<br>63.4 | 61.1<br>73.1 | 60.1<br>76.5 |
| **Coding**          |           Livecodebench<br>Fullstackbench            | 11.1<br>20.9 | 31.5<br>42   | 49.4<br>54.6 | 57<br>56.3 |
| **Reasoning**       |              BBH<br>DROP<br>ZebraLogic               | 40.3<br>52.8<br>34.5 | 64.6<br>76.7<br>74.6 | 83<br>78.2<br>83.5 | 87.8<br>85.9<br>85.1 |
| **Instruction<br>Following** |        IF-Eval<br>SysBench                  | 49.7<br>28.1 | 67.6<br>55.5 | 76.6<br>68 | 79.3<br>72.7 |
| **Agent**           | BFCL v3<br> Ï„-Bench<br>ComplexFuncBench<br> C3-Bench | 49.8<br>14.4<br>13.9<br>45.3 | 58.3<br>18.2<br>22.3<br>54.6 | 67.9<br>30.1<br>26.3<br>64.3 | 70.8<br>35.3<br>29.2<br>68.5 |
| **Long<br>Context** | PenguinScrolls<br>longbench-v2<br>FRAMES          | 53.9<br>34.7<br>41.9 | 73.1<br>33.2<br>55.6 | 83.1<br>44.1<br>79.2 | 82<br>43<br>78.6 |

&nbsp;

## ä½¿ç”¨ transformers æ¨ç†
é¦–å…ˆï¼Œéœ€è¦å®‰è£…æŒ‡å®šç‰ˆæœ¬çš„transformersï¼Œæˆ‘ä»¬å°†åœ¨ä¸ä¹…åå®Œæˆå¯¹transformersä¸»åˆ†æ”¯çš„åˆå…¥
```SHELL
pip install git+https://github.com/huggingface/transformers@4970b23cedaf745f963779b4eae68da281e8c6ca
```
æˆ‘ä»¬çš„æ¨¡å‹é»˜è®¤ä½¿ç”¨æ…¢æ€è€ƒè¿›è¡Œæ¨ç†ï¼Œæœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥ç¦ç”¨ CoT æ¨ç†ã€‚

1. è°ƒç”¨ apply_chat_template æ—¶ä¼ é€’ **enable_thinking=False**ã€‚
2. åœ¨ prompt å‰æ·»åŠ  **/no_think** å°†ä¼šå¼ºåˆ¶æ¨¡å‹ä¸ä½¿ç”¨ CoT æ¨ç†ã€‚åŒç†ï¼Œåœ¨ prompt å‰æ·»åŠ  **/think** å°†ä¼šå¼ºåˆ¶æ¨¡å‹æ‰§è¡Œ CoT æ¨ç†ã€‚

ä»¥ä¸‹ä»£ç ç‰‡æ®µå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ transformers åº“åŠ è½½å’Œä½¿ç”¨æ¨¡å‹ã€‚å®ƒè¿˜æ¼”ç¤ºäº†å¦‚ä½•ç¦ç”¨æ¨ç†æ¨¡å¼ï¼Œä»¥åŠå¦‚ä½•è§£æå‡ºâ€œæ¨ç†è¿‡ç¨‹â€å’Œâ€œæœ€ç»ˆè¾“å‡ºâ€ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import os
import re

model_name_or_path = os.environ['MODEL_PATH']
# model_name_or_path = "tencent/Hunyuan-7B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map="auto",trust_remote_code=True)  # You may want to use bfloat16 and/or move to GPU here
messages = [
    {"role": "user", "content": "Write a short summary of the benefits of regular exercise"},
]
tokenized_chat = tokenizer.apply_chat_template(
    messages,
    tokenize=False
    add_generation_prompt=True,
    enable_thinking=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
model_inputs.pop("token_type_ids", None)
outputs = model.generate(**model_inputs, max_new_tokens=4096)
output_text = tokenizer.decode(outputs[0])

think_pattern = r'<think>(.*?)</think>'
think_matches = re.findall(think_pattern, output_text, re.DOTALL)

answer_pattern = r'<answer>(.*?)</answer>'
answer_matches = re.findall(answer_pattern, output_text, re.DOTALL)

think_content = [match.strip() for match in think_matches][0]
answer_content = [match.strip() for match in answer_matches][0]
print(f"thinking_content:{think_content}\n\n")
print(f"answer_content:{answer_content}\n\n")
```


æˆ‘ä»¬æ¨èä½¿ç”¨ä¸‹é¢è¿™ç»„å‚æ•°è¿›è¡Œæ¨ç†ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ²¡æœ‰é»˜è®¤ system_promptã€‚

```json

{
  "do_sample": true,
  "top_k": 20,
  "top_p": 0.8,
  "repetition_penalty": 1.05,
  "temperature": 0.7
}
```

&nbsp;

## è®­ç»ƒæ•°æ®æ ¼å¼å¤„ç†

å¦‚æœéœ€è¦å¾®è°ƒæˆ‘ä»¬çš„ Instruct æ¨¡å‹ï¼Œå»ºè®®å°†æ•°æ®å¤„ç†æˆä»¥ä¸‹æ ¼å¼ï¼Œåˆ†åˆ«å¯¹åº”æ…¢æ€è€ƒå’Œå¿«æ€è€ƒçš„åœºæ™¯ã€‚

```python
# think_pattern
think = ""
answer = ""
think_pattern = f"<think>\n{think}\n</think>\n<answer>\n{answer}\n</answer>"

# fast think pattern
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "/no_thinkæµ·æ°´ä¸ºä»€ä¹ˆæ˜¯å’¸çš„" },
    {"role": "assistant", "content": "<think>\n\n</think>\n<answer>\næµ·æ°´æ˜¯å’¸çš„ä¸»è¦æ˜¯å› ä¸ºå…¶ä¸­å«æœ‰è®¸å¤šæº¶è§£åœ¨æ°´ä¸­çš„ç›ç±»å’ŒçŸ¿ç‰©è´¨ã€‚è¿™äº›ç›ç±»å’ŒçŸ¿ç‰©è´¨æ¥è‡ªäºåœ°çƒè¡¨é¢çš„å²©çŸ³å’ŒåœŸå£¤ä¸­çš„åŒ–å­¦ç‰©è´¨ï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œå®ƒä»¬è¢«å¸¦åˆ°äº†æµ·æ´‹ä¸­ã€‚å½“æµ·æ°´è’¸å‘æ—¶ï¼Œæ°´åˆ†è’¸å‘æ‰äº†ï¼Œä½†ç›ç±»å’ŒçŸ¿ç‰©è´¨ä»ç„¶ç•™åœ¨æ°´ä¸­ï¼Œå¯¼è‡´æµ·æ°´å˜å¾—æ›´åŠ å’¸å‘³ã€‚å› æ­¤ï¼Œæµ·æ°´çš„å’¸åº¦æ˜¯ç”±å…¶ä¸­çš„ç›ç±»å’ŒçŸ¿ç‰©è´¨çš„å«é‡å†³å®šçš„ã€‚\n</answer>"}
]

# slow think pattern
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "1+1=" },
    {"role": "assistant", "content": "<think>\nå—¯ï¼Œç”¨æˆ·é—®çš„æ˜¯1åŠ 1ç­‰äºå¤šå°‘ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤è¿™æ˜¯ä¸€ä¸ªåŸºæœ¬çš„ç®—æœ¯é—®é¢˜ã€‚1åŠ 1åœ¨åè¿›åˆ¶çš„æ•°å­¦ä½“ç³»ä¸­ï¼Œé€šå¸¸çš„ç»“æœæ˜¯2ã€‚ä¸è¿‡ï¼Œå¯èƒ½éœ€è¦è€ƒè™‘æ˜¯å¦æœ‰å…¶ä»–æƒ…å†µï¼Œæ¯”å¦‚äºŒè¿›åˆ¶æˆ–è€…å…¶ä»–æ•°åˆ¶ï¼Œä½†ç”¨æˆ·æ²¡æœ‰ç‰¹åˆ«è¯´æ˜ï¼Œæ‰€ä»¥é»˜è®¤åº”è¯¥æ˜¯åè¿›åˆ¶ã€‚å¦å¤–ï¼Œæœ‰æ—¶å€™å¯èƒ½ä¼šæœ‰è„‘ç­‹æ€¥è½¬å¼¯çš„æƒ…å†µï¼Œæ¯”å¦‚åœ¨æŸäº›è¯­å¢ƒä¸‹1+1å¯èƒ½ç­‰äº1ï¼ˆæ¯”å¦‚1æ»´æ°´åŠ 1æ»´æ°´è¿˜æ˜¯1æ»´æ°´ï¼‰ï¼Œä½†é€šå¸¸æ•°å­¦é—®é¢˜ä¸­éƒ½æ˜¯2ã€‚æ‰€ä»¥æœ€å‡†ç¡®çš„å›ç­”åº”è¯¥æ˜¯2ã€‚</think>\n<answer>\nåœ¨åè¿›åˆ¶çš„åŸºæœ¬ç®—æœ¯è¿ç®—ä¸­ï¼Œ1åŠ 1çš„ç»“æœæ˜¯2ã€‚è¿™æ˜¯æ•°å­¦ä¸­æœ€åŸºç¡€çš„åŠ æ³•è¿ç®—ä¹‹ä¸€ï¼Œéµå¾ªè‡ªç„¶æ•°çš„åŠ æ³•è§„åˆ™ã€‚å› æ­¤ï¼Œ1 + 1 = 2ã€‚\n</answer>"}
]

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("your_tokenizer_path", trust_remote_code=True)
train_ids = tokenizer.apply_chat_template(messages)
```

&nbsp;

## ä½¿ç”¨ LLaMA-Factory è®­ç»ƒ

æˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨`LLaMA-Factory`æ¥è¿›è¡Œå¾®è°ƒæ··å…ƒæ¨¡å‹ã€‚

### å®‰è£…ç¯å¢ƒ

å¼€å§‹ä¹‹å‰ï¼Œç¡®ä¿ä½ å·²ç»å®‰è£…äº†ä»¥ä¸‹ä»£ç åº“ï¼š
1. ä½¿ç”¨[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)å®˜æ–¹æŒ‡å¯¼è¿›è¡Œå®‰è£…ã€‚
2. ä½¿ç”¨[DeepSpeed](https://github.com/deepspeedai/DeepSpeed#installation)å®˜æ–¹æŒ‡å¯¼è¿›è¡Œå®‰è£…ï¼ˆå¯é€‰ï¼‰ã€‚
3. å®‰è£…é…å¥—çš„transformeråº“ã€‚å½“å‰æ··å…ƒæäº¤çš„transformerä»£ç æ­£åœ¨è¯„å®¡ä¸­ï¼Œéœ€è¦è·å–é…å¥—çš„åˆ†æ”¯ã€‚
```
pip install git+https://github.com/huggingface/transformers@4970b23cedaf745f963779b4eae68da281e8c6ca
```

### å‡†å¤‡æ•°æ®

æˆ‘ä»¬éœ€è¦å‡†å¤‡è‡ªå®šä¹‰çš„æ•°æ®é›†ï¼š

1. è¯·å°†æ‚¨çš„æ•°æ®ä»¥`json`æ ¼å¼è¿›è¡Œç»„ç»‡ï¼Œå¹¶å°†æ•°æ®æ”¾å…¥`LLaMA-Factory`çš„`data`ç›®å½•ä¸­ã€‚å½“å‰ä½¿ç”¨çš„æ˜¯`sharegpt`æ ¼å¼çš„æ•°æ®é›†ï¼Œéœ€è¦éµå¾ªä»¥ä¸‹æ ¼å¼ï¼š
```
[
  {
    "messages": [
      {
        "role": "system",
        "content": "ç³»ç»Ÿæç¤ºè¯ï¼ˆé€‰å¡«ï¼‰"
      },
      {
        "role": "user",
        "content": "äººç±»æŒ‡ä»¤"
      },
      {
        "role": "assistant",
        "content": "æ¨¡å‹å›ç­”"
      }
    ]
  }
]
```
å¯ä»¥å‚è€ƒå‰é¢ç« èŠ‚ä¸­å¯¹[æ•°æ®æ ¼å¼](#è®­ç»ƒæ•°æ®æ ¼å¼å¤„ç†)çš„è¯´æ˜ã€‚

2. åœ¨`data/dataset_info.json`æ–‡ä»¶ä¸­æä¾›æ‚¨çš„æ•°æ®é›†å®šä¹‰ï¼Œå¹¶é‡‡ç”¨ä»¥ä¸‹æ ¼å¼ï¼š
```
"æ•°æ®é›†åç§°": {
  "file_name": "data.json",
  "formatting": "sharegpt",
  "columns": {
    "messages": "messages"
  },
  "tags": {
    "role_tag": "role",
    "content_tag": "content",
    "user_tag": "user",
    "assistant_tag": "assistant",
    "system_tag": "system"
  }
}
```

### è®­ç»ƒ

1. å°†`llama_factory_support/example_configs`ç›®å½•ä¸‹çš„æ–‡ä»¶éƒ½æ‹·è´åˆ°`LLaMA-Factory`çš„`example/hunyuan`ç›®å½•ä¸‹ã€‚
2. ä¿®æ”¹é…ç½®æ–‡ä»¶`hunyuan_full.yaml`ä¸­çš„æ¨¡å‹è·¯å¾„å’Œæ•°æ®é›†åç§°ï¼Œå…¶ä»–çš„é…ç½®è¯·æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚
  ```
  ### model
  model_name_or_path: [!!!add the model path here!!!]

  ### dataset
  dataset: [!!!add the data set name here!!!]
  ```
3. æ‰§è¡Œè®­ç»ƒå‘½ä»¤
    * è¿è¡Œå•æœºè®­ç»ƒ
    è¯·æ³¨æ„è¿™é‡Œéœ€è¦è®¾ç½®`DISABLE_VERSION_CHECK`ç¯å¢ƒå˜é‡ï¼Œé¿å…ç‰ˆæœ¬å†²çªã€‚
    ```
    export DISABLE_VERSION_CHECK=1
    llamafactory-cli train examples/hunyuan/hunyuan_full.yaml
    ```
    * è¿è¡Œå¤šæœºè®­ç»ƒ
    åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ã€‚è¯·æ³¨æ„å°†`torchrun`éœ€è¦çš„`NNODES`ã€`NODE_RANK`ã€`MASTER_ADDR`å’Œ`MASTER_PORT`æŒ‰ç…§æ‚¨è¿è¡Œçš„ç¯å¢ƒè¿›è¡Œé…ç½®ã€‚
    ```
    export DISABLE_VERSION_CHECK=1
    FORCE_TORCHRUN=1 NNODES=${NNODES} NODE_RANK=${NODE_RANK} MASTER_ADDR=${MASTER_ADDR} MASTER_PORT=${MASTER_PORT} \
    llamafactory-cli train examples/hunyuan_full.yaml
    ```

&nbsp;

## é‡åŒ–å‹ç¼©

æˆ‘ä»¬ä½¿ç”¨äº† [AngleSlim](https://github.com/tencent/AngelSlim) å‹ç¼©å·¥å…·æ¥ç”Ÿæˆ FP8 å’Œ INT4 é‡åŒ–æ¨¡å‹ã€‚`AngleSlim` æ˜¯ä¸€æ¬¾ä¸“é—¨è‡´åŠ›äºæ‰“é€ æ›´æ˜“ç”¨ã€æ›´å…¨é¢ä¸”æ›´é«˜æ•ˆçš„æ¨¡å‹å‹ç¼©è§£å†³æ–¹æ¡ˆçš„å·¥å…·ã€‚

### FP8 é‡åŒ–
æˆ‘ä»¬é‡‡ç”¨FP8-staticé‡åŒ–ï¼ŒFP8é‡åŒ–é‡‡ç”¨8ä½æµ®ç‚¹æ ¼å¼ï¼Œé€šè¿‡å°‘é‡æ ¡å‡†æ•°æ®ï¼ˆæ— éœ€è®­ç»ƒï¼‰é¢„å…ˆç¡®å®šé‡åŒ–scaleï¼Œå°†æ¨¡å‹æƒé‡ä¸æ¿€æ´»å€¼è½¬æ¢ä¸ºFP8æ ¼å¼ï¼Œæå‡æ¨ç†æ•ˆç‡å¹¶é™ä½éƒ¨ç½²é—¨æ§›ã€‚ æˆ‘ä»¬æ‚¨å¯ä»¥ä½¿ç”¨AngleSlimé‡åŒ–ï¼Œä½ ä¹Ÿå¯ä»¥ç›´æ¥ä¸‹è½½æˆ‘ä»¬é‡åŒ–å®Œæˆçš„å¼€æºæ¨¡å‹ä½¿ç”¨ [AngelSlim](https://huggingface.co/AngelSlim).

### Int4 Quantization
Int4é‡åŒ–æˆ‘ä»¬é‡‡ç”¨GPTQå’ŒAWQç®—æ³•å®ç°W4A16é‡åŒ–ã€‚

GPTQç®—æ³•é‡‡ç”¨é€å±‚å¤„ç†æ¨¡å‹æƒé‡ï¼Œåˆ©ç”¨å°‘é‡æ ¡å‡†æ•°æ®æœ€å°åŒ–é‡åŒ–åçš„æƒé‡é‡æ„è¯¯å·®ï¼Œé€šè¿‡è¿‘ä¼¼Hessiané€†çŸ©é˜µçš„ä¼˜åŒ–è¿‡ç¨‹é€å±‚è°ƒæ•´æƒé‡ã€‚æµç¨‹æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œä»…éœ€å°‘é‡æ ¡å‡†æ•°æ®å³å¯é‡åŒ–æƒé‡ï¼Œæå‡æ¨ç†æ•ˆç‡å¹¶é™ä½éƒ¨ç½²é—¨æ§›ã€‚
AWQä½¿ç”¨å°‘é‡æ ¡å‡†æ•°æ®ï¼ˆæ— éœ€è¿›è¡Œè®­ç»ƒï¼‰æ¥è®¡ç®—æ¿€æ´»å€¼çš„å¹…åº¦ï¼Œä»è€Œè¿›è¡Œç»Ÿè®¡è®¡ç®—ã€‚å¯¹äºæ¯ä¸ªæƒé‡é€šé“ï¼Œéƒ½ä¼šè®¡ç®—ä¸€ä¸ªç¼©æ”¾ç³»æ•°sï¼Œä»¥æ‰©å¤§é‡è¦æƒé‡çš„æ•°å€¼è¡¨è¾¾èŒƒå›´ï¼Œä»è€Œåœ¨é‡åŒ–è¿‡ç¨‹ä¸­èƒ½å¤Ÿä¿ç•™æ›´å¤šçš„ä¿¡æ¯ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ [AngleSlim](https://github.com/tencent/AngelSlim) é‡åŒ–ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä¸‹è½½æˆ‘ä»¬é‡åŒ–å®Œæˆçš„å¼€æºæ¨¡å‹ä½¿ç”¨ [AngelSlim](https://huggingface.co/AngelSlim) ã€‚


#### é‡åŒ– Benchmark
æœ¬å°èŠ‚ä»‹ç»äº†æ··å…ƒé‡åŒ–æ¨¡å‹çš„åŸºå‡†æŒ‡æ ‡ã€‚

|     Bench     |           Quantization            |    Hunyuan-0.5B-Instruct     |     Hunyuan-1.8B-Instruct      |     Hunyuan-4B-Instruct      |     Hunyuan-7B-Instruct      |
|:-------------:|:---------------------------------:|:----------------------------:|:------------------------------:|:----------------------------:|:----------------------------:|
|     DROP      | B16<br>FP8<br>Int4GPTQ<br>Int4AWQ | 52.8<br>51.6<br>50.9<br>48.9 |  76.7<br>75.1<br>73.0<br>71.7  | 78.2<br>78.3<br>78.1<br>78.2 | 85.9<br>86.0<br>85.7<br>85.9 |
| GPQA-Diamond  | B16<br>FP8<br>Int4GPTQ<br>Int4AWQ | 23.3<br>22.5<br>23.3<br>23.3 | 47.2<br>47.7<br>44.43<br>43.62 |  61.1<br>60.2<br>58.1<br>-   | 60.1<br>60.1<br>60.0<br>60.1 |
| OlympiadBench | B16<br>FP8<br>Int4GPTQ<br>Int4AWQ | 29.6<br>29.6<br>26.8<br>26.3 |  63.4<br>62.5<br>60.9<br>61.7  | 73.1<br>73.1<br>72.9<br>72.8 | 76.5<br>76.6<br>76.2<br>76.4 |
|   AIME 2024   | B16<br>FP8<br>Int4GPTQ<br>Int4AWQ |    17.2<br>17.2<br>-<br>-    |    56.7<br>55.17<br>-<br>-     |    78.3<br>76.6<br>-<br>-    | 81.1<br>80.9<br>81.0<br>80.9 |



&nbsp;

## æ¨ç†å’Œéƒ¨ç½²

HunyuanLLMå¯ä»¥é‡‡ç”¨TensorRT-LLM, vLLMæˆ–sglangéƒ¨ç½²ã€‚ä¸ºäº†ç®€åŒ–éƒ¨ç½²è¿‡ç¨‹HunyuanLLMæä¾›äº†é¢„æ„å»ºdockeré•œåƒï¼Œè¯¦è§ä¸€ä¸‹ç« èŠ‚ã€‚

é•œåƒï¼šhttps://hub.docker.com/r/hunyuaninfer/hunyuan-7b/tags

## ä½¿ç”¨TensorRT-LLMæ¨ç†
### Docker:

ä¸ºäº†ç®€åŒ–éƒ¨ç½²è¿‡ç¨‹ï¼ŒHunyuanLLMæä¾›äº†é¢„æ„å»ºdockeré•œåƒ (æ³¨æ„ï¼š è¯¥é•œåƒè¦æ±‚Hostçš„Cudaç‰ˆæœ¬ä¸º12.8ä»¥ä¸Šï¼‰ï¼š

[hunyuaninfer/hunyuan-7b:hunyuan-7b-trtllm](https://hub.docker.com/r/hunyuaninfer/hunyuan-7b/tags) ã€‚æ‚¨åªéœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶å¹¶ç”¨ä¸‹é¢ä»£ç å¯åŠ¨dockerå³å¯å¼€å§‹æ¨ç†æ¨¡å‹ã€‚
```shell
# æ‹‰å–
å›½å†…ï¼š
docker pull docker.cnb.cool/tencent/hunyuan/hunyuan-7b:hunyuan-7b-trtllm
å›½å¤–ï¼š
docker pull hunyuaninfer/hunyuan-7b:hunyuan-7b-trtllm

# å¯åŠ¨
docker run --privileged --user root --name hunyuanLLM_infer --rm -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --gpus=all hunyuaninfer/hunyuan-7b:hunyuan-7b-trtllm
```

æ³¨: Dockerå®¹å™¨æƒé™ç®¡ç†ã€‚ä»¥ä¸Šä»£ç é‡‡ç”¨ç‰¹æƒæ¨¡å¼ï¼ˆ--privilegedï¼‰å¯åŠ¨Dockerå®¹å™¨ä¼šèµ‹äºˆå®¹å™¨è¾ƒé«˜çš„æƒé™ï¼Œå¢åŠ æ•°æ®æ³„éœ²å’Œé›†ç¾¤å®‰å…¨é£é™©ã€‚å»ºè®®åœ¨éå¿…è¦æƒ…å†µä¸‹é¿å…ä½¿ç”¨ç‰¹æƒæ¨¡å¼ï¼Œä»¥é™ä½å®‰å…¨å¨èƒã€‚å¯¹äºå¿…é¡»ä½¿ç”¨ç‰¹æƒæ¨¡å¼çš„åœºæ™¯ï¼Œåº”è¿›è¡Œä¸¥æ ¼çš„å®‰å…¨è¯„ä¼°ï¼Œå¹¶å®æ–½ç›¸åº”çš„å®‰å…¨ç›‘æ§ã€åŠ å›ºæªæ–½ã€‚

### BF16éƒ¨ç½²

#### Step1ï¼šæ‰§è¡Œæ¨ç†

#### æ–¹å¼1ï¼šå‘½ä»¤è¡Œæ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä¸€ä¸ªä»£ç ç‰‡æ®µï¼Œé‡‡ç”¨`TensorRT-LLM`å¿«é€Ÿè¯·æ±‚chat modelï¼š
ä¿®æ”¹ examples/pytorch/quickstart_advanced.py ä¸­å¦‚ä¸‹ä»£ç ï¼š


```python
def setup_llm(args):
    kv_cache_config = KvCacheConfig(
        enable_block_reuse=not args.disable_kv_cache_reuse,
        free_gpu_memory_fraction=args.kv_cache_fraction,
    )
    spec_config = None

    hf_ckpt_path="$your_hunyuan_model_path"
    tokenizer = AutoTokenizer.from_pretrained(hf_ckpt_path, trust_remote_code=True)
    llm = LLM(
        tokenizer=tokenizer,
        model=args.model_dir,
        backend='pytorch',
        disable_overlap_scheduler=args.disable_overlap_scheduler,
        kv_cache_dtype=args.kv_cache_dtype,
        kv_cache_config=kv_cache_config,
        attn_backend=args.attention_backend,
        use_cuda_graph=args.use_cuda_graph,
        cuda_graph_padding_enabled=args.cuda_graph_padding_enabled,
        cuda_graph_batch_sizes=args.cuda_graph_batch_sizes,
        load_format=args.load_format,
        print_iter_log=args.print_iter_log,
        enable_iter_perf_stats=args.print_iter_log,
        torch_compile_config=TorchCompileConfig(
            enable_fullgraph=args.use_torch_compile,
            enable_inductor=args.use_torch_compile,
            enable_piecewise_cuda_graph= \
                args.use_piecewise_cuda_graph)
        if args.use_torch_compile else None,
        moe_backend=args.moe_backend,
        enable_trtllm_sampler=args.enable_trtllm_sampler,
        max_seq_len=args.max_seq_len,
        max_batch_size=args.max_batch_size,
        max_num_tokens=args.max_num_tokens,
        enable_attention_dp=args.enable_attention_dp,
        tensor_parallel_size=args.tp_size,
        pipeline_parallel_size=args.pp_size,
        moe_expert_parallel_size=args.moe_ep_size,
        moe_tensor_parallel_size=args.moe_tp_size,
        moe_cluster_parallel_size=args.moe_cluster_size,
        enable_chunked_prefill=args.enable_chunked_prefill,
        speculative_config=spec_config,
        trust_remote_code=args.trust_remote_code,
        gather_generation_logits=args.return_generation_logits)

    sampling_params = SamplingParams(
        end_id=127960,
        max_tokens=args.max_tokens,
        temperature=args.temperature,
        top_k=args.top_k,
        top_p=args.top_p,
        return_context_logits=args.return_context_logits,
        return_generation_logits=args.return_generation_logits,
        logprobs=args.logprobs)
    return llm, sampling_params


def main():
    args = parse_arguments()
    prompts = args.prompt if args.prompt else example_prompts

    llm, sampling_params = setup_llm(args)
    new_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": f"{prompt}"}]
        new_prompts.append(
            llm.tokenizer.apply_chat_template(messages,
                                                tokenize=False,
                                                add_generation_prompt=True))
    prompts = new_prompts
    outputs = llm.generate(prompts, sampling_params)

    for i, output in enumerate(outputs):
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f"[{i}] Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

è¿è¡Œæ–¹å¼ï¼š

```shell
python3 quickstart_advanced.py --model_dir "HunyuanLLMæ¨¡å‹è·¯å¾„" --tp_size 1
```

#### æ–¹å¼2ï¼šæœåŠ¡åŒ–æ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä½¿ç”¨`TensorRT-LLM`æœåŠ¡åŒ–çš„æ–¹å¼éƒ¨ç½²æ¨¡å‹å’Œè¯·æ±‚ã€‚

ä»¥tencent/Hunyuan-7B-Instructä¸ºä¾‹
å‡†å¤‡é…ç½®æ–‡ä»¶ï¼š

```
cat >/path/to/extra-llm-api-config.yml <<EOF
use_cuda_graph: true
cuda_graph_padding_enabled: true
cuda_graph_batch_sizes:
- 1
- 2
- 4
- 8
- 16
- 32
print_iter_log: true
EOF
```

å¯åŠ¨æœåŠ¡ï¼š

```shell
trtllm-serve \
  /path/to/HunYuan-7b \
  --host localhost \
  --port 8000 \
  --backend pytorch \
  --max_batch_size 32 \
  --max_num_tokens 16384 \
  --tp_size 1 \
  --kv_cache_free_gpu_memory_fraction 0.6 \
  --trust_remote_code \
  --extra_llm_api_options /path/to/extra-llm-api-config.yml
```

æœåŠ¡å¯åŠ¨æˆåŠŸå, ä½¿ç”¨ OpenAI API è¿›è¡Œæ¨¡å‹æ¨ç†è°ƒç”¨ï¼š
```
curl -X POST "http://localhost:8000/v1/chat/completions" \
  -H "Content-Type: application/json" \
  --data '{
    "model": "HunYuan/HunYuan-7b",
    "messages": [
      {
        "role": "user",
        "content": "Write a short summary of the benefits of regular exercise"
      }
    ]
  }'
```

#### FP8/Int4é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼š
ç›®å‰ TensorRT-LLM çš„ fp8 å’Œ int4 é‡åŒ–æ¨¡å‹æ­£åœ¨æ”¯æŒä¸­ï¼Œæ•¬è¯·æœŸå¾…ã€‚


## ä½¿ç”¨vLLMæ¨ç†
### ç‰ˆæœ¬è¦æ±‚:

è¯·ä½¿ç”¨vLLM v0.10.0ä¹‹åçš„ç‰ˆæœ¬è¿›è¡Œéƒ¨ç½²å’Œæ¨ç†

éœ€è¦å®‰è£…æŒ‡å®šç‰ˆæœ¬çš„transformersï¼Œæˆ‘ä»¬å°†åœ¨ä¸ä¹…åå®Œæˆå¯¹transformersä¸»åˆ†æ”¯çš„åˆå…¥
```SHELL
pip install git+https://github.com/huggingface/transformers@4970b23cedaf745f963779b4eae68da281e8c6ca
```
### BF16éƒ¨ç½²

ä»¥tencent/Hunyuan-7B-Instructä¸ºä¾‹ï¼Œå·²ç»é€šè¿‡ä¸Šè¿°çš„transformersè·å–äº†æ¨¡å‹åœ°å€
```shell
export MODEL_PATH=PATH_TO_MODEL
```

#### Step1ï¼šæ‰§è¡Œæ¨ç†

#### æ–¹å¼1ï¼šå‘½ä»¤è¡Œæ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä¸€ä¸ªä»£ç ç‰‡æ®µï¼Œé‡‡ç”¨`vLLM`å¿«é€Ÿè¯·æ±‚chat modelï¼š

æ³¨: vLLMç»„ä»¶è¿œç¨‹ä»£ç æ‰§è¡Œé˜²æŠ¤ã€‚ä¸‹åˆ—ä»£ç ä¸­vLLMç»„ä»¶çš„trust-remote-codeé…ç½®é¡¹è‹¥è¢«å¯ç”¨ï¼Œå°†å…è®¸åŠ è½½å¹¶æ‰§è¡Œæ¥è‡ªè¿œç¨‹æ¨¡å‹ä»“åº“çš„ä»£ç ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¶æ„ä»£ç çš„æ‰§è¡Œã€‚é™¤éä¸šåŠ¡éœ€æ±‚æ˜ç¡®è¦æ±‚ï¼Œå¦åˆ™å»ºè®®è¯¥é…ç½®é¡¹å¤„äºç¦ç”¨çŠ¶æ€ï¼Œä»¥é™ä½æ½œåœ¨çš„å®‰å…¨å¨èƒã€‚


```python
import os
from typing import List, Optional
from vllm import LLM, SamplingParams
from vllm.inputs import PromptType
from transformers import AutoTokenizer

model_path=os.environ.get('MODEL_PATH')
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

llm = LLM(model=model_path,
        tokenizer=model_path,
        trust_remote_code=True,
        dtype='bfloat16',
        tensor_parallel_size=4,
        gpu_memory_utilization=0.9)

sampling_params = SamplingParams(
    temperature=0.7, top_p=0.8, max_tokens=4096, top_k=20, repetition_penalty=1.05)

messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant.",
    },
    {"role": "user", "content": "Write a short summary of the benefits of regular exercise"},
]

tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")

dummy_inputs: List[PromptType] = [{
    "prompt_token_ids": batch
} for batch in tokenized_chat.numpy().tolist()]

outputs = llm.generate(dummy_inputs, sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

#### æ–¹å¼2ï¼šæœåŠ¡åŒ–æ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä½¿ç”¨`vLLM`æœåŠ¡åŒ–çš„æ–¹å¼éƒ¨ç½²æ¨¡å‹å¹¶è¯·æ±‚

æˆ‘ä»¬å¯åŠ¨æœåŠ¡ï¼Œè¿è¡Œ :
```shell
python3 -m vllm.entrypoints.openai.api_server \
    --host 0.0.0.0 \
    --port 8000 \
    --trust-remote-code \
    --model ${MODEL_PATH} \
    --tensor-parallel-size 1 \
    --dtype bfloat16 \
    --quantization experts_int8 \
    --served-model-name hunyuan \
    2>&1 | tee log_server.txt
```

è¿è¡ŒæˆåŠŸå, è¿è¡Œè¯·æ±‚è„šæœ¬ï¼š
```shell
curl http://0.0.0.0:8000/v1/chat/completions -H 'Content-Type: application/json' -d '{
"model": "hunyuan",
"messages": [
    {
        "role": "system",
        "content": [{"type": "text", "text": "You are a helpful assistant."}]
    },
    {
        "role": "user",
        "content": [{"type": "text", "text": "è¯·æŒ‰é¢ç§¯å¤§å°å¯¹å››å¤§æ´‹è¿›è¡Œæ’åºï¼Œå¹¶ç»™å‡ºé¢ç§¯æœ€å°çš„æ´‹æ˜¯å“ªä¸€ä¸ªï¼Ÿç›´æ¥è¾“å‡ºç»“æœã€‚"}]
    }
],
"max_tokens": 2048,
"temperature":0.7,
"top_p": 0.6,
"top_k": 20,
"repetition_penalty": 1.05,
"stop_token_ids": [127960]
}'
```

### é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼š

æœ¬éƒ¨åˆ†ä»‹ç»é‡‡ç”¨vLLMéƒ¨ç½²é‡åŒ–åæ¨¡å‹çš„æµç¨‹ã€‚


#### Int8é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼š
éƒ¨ç½²Int8-weight-onlyç‰ˆæœ¬HunYuan-7Bæ¨¡å‹

æˆ‘ä»¬å¯åŠ¨Int8æœåŠ¡ï¼Œè¿è¡Œï¼š
```shell
python3 -m vllm.entrypoints.openai.api_server \
    --host 0.0.0.0 \
    --port 8000 \
    --trust-remote-code \
    --model ${MODEL_PATH} \
    --tensor-parallel-size 1 \
    --dtype bfloat16 \
    --served-model-name hunyuan \
    --quantization experts_int8 \
    2>&1 | tee log_server.txt
```

#### Int4é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼š
éƒ¨ç½²Int4-weight-onlyç‰ˆæœ¬HunYuan-7Bæ¨¡å‹ï¼Œé‡‡ç”¨GPTQæ–¹å¼ï¼š

```shell
export MODEL_PATH=PATH_TO_INT4_MODEL
```
æ¥ç€æˆ‘ä»¬å¯åŠ¨Int4æœåŠ¡ï¼Œè¿è¡Œï¼š
```shell
python3 -m vllm.entrypoints.openai.api_server \
    --host 0.0.0.0 \
    --port 8000 \
    --trust-remote-code \
    --model ${MODEL_PATH} \
    --tensor-parallel-size 1 \
    --dtype bfloat16 \
    --served-model-name hunyuan \
    --quantization gptq_marlin \
    2>&1 | tee log_server.txt
```


#### FP8é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼š
éƒ¨ç½²W8A8C8ç‰ˆæœ¬HunYuan-7Bæ¨¡å‹

æˆ‘ä»¬å¯åŠ¨FP8æœåŠ¡ï¼Œè¿è¡Œï¼š
```shell
python3 -m vllm.entrypoints.openai.api_server \
    --host 0.0.0.0 \
    --port 8000 \
    --trust-remote-code \
    --model ${MODEL_PATH} \
    --tensor-parallel-size 1 \
    --dtype bfloat16 \
    --served-model-name hunyuan \
    --kv-cache-dtype fp8 \
    2>&1 | tee log_server.txt
```

## ä½¿ç”¨sglangæ¨ç†

### BF16éƒ¨ç½²

#### Step1: æ‹‰å–é•œåƒ


```
docker pull lmsysorg/sglang:latest
```

- å¯åŠ¨ API server:

```
docker run --entrypoint="python3" --gpus all \
    --shm-size 32g \
    -p 30000:30000 \
    --ulimit nproc=10000 \
    --privileged \
    --ipc=host \
     lmsysorg/sglang:latest \
    -m sglang.launch_server --model-path hunyuan/huanyuan_7B --tp 1 --trust-remote-code --host 0.0.0.0 --port 30000
```

#### Step2ï¼šæ‰§è¡Œæ¨ç†

#### æ–¹å¼1ï¼šå‘½ä»¤è¡Œæ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä¸€ä¸ªä»£ç ç‰‡æ®µï¼Œé‡‡ç”¨`sglang`å¿«é€Ÿè¯·æ±‚chat modelï¼š


```python
import sglang as sgl
from transformers import AutoTokenizer

model_path=os.environ.get('MODEL_PATH')


tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant.",
    },
    {"role": "user", "content": "Write a short summary of the benefits of regular exercise"},
]
prompts = []
prompts.append(tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
))
print(prompts)

llm = sgl.Engine(
    model_path=model_path,
    tp_size=1,
    trust_remote_code=True,
    mem_fraction_static=0.7,
)

sampling_params = {"temperature": 0.7, "top_p": 0.8, "top_k": 20, "max_new_tokens": 4096}
outputs = llm.generate(prompts, sampling_params)
for prompt, output in zip(prompts, outputs):
    print(f"Prompt: {prompt}\nGenerated text: {output['text']}")
```

#### æ–¹å¼2ï¼šæœåŠ¡åŒ–æ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä½¿ç”¨`sglang`æœåŠ¡åŒ–çš„æ–¹å¼éƒ¨ç½²æ¨¡å‹å’Œè¯·æ±‚ã€‚

```shell
model_path="HunyuanLLMæ¨¡å‹è·¯å¾„"
python3 -u -m sglang.launch_server \
    --model-path $model_path \
    --tp 4 \
    --trust-remote-code
```

æœåŠ¡å¯åŠ¨æˆåŠŸå, è¿è¡Œè¯·æ±‚è„šæœ¬ï¼š
```python
import openai
client = openai.Client(
    base_url="http://localhost:30000/v1", api_key="EMPTY")

response = client.chat.completions.create(
    model="default",
    messages= [
        {"role": "user", "content": "Write a short summary of the benefits of regular exercise"},
    ],
    temperature=0.7,
    max_tokens=4096,
    extra_body={"top_p": 0.8, "top_k": 20}
)
print(response)
```

#### FP8/Int4é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼š
ç›®å‰ sglang çš„ fp8 å’Œ int4 é‡åŒ–æ¨¡å‹æ­£åœ¨æ”¯æŒä¸­ï¼Œæ•¬è¯·æœŸå¾…ã€‚

## äº¤äº’å¼Demo Web
hunyuan-7B ç°å·²å¼€æ”¾ç½‘é¡µdemoã€‚è®¿é—® https://hunyuan.tencent.com/?model=hunyuan-7b å³å¯ç®€å•ä½“éªŒæˆ‘ä»¬çš„æ¨¡å‹ã€‚

## ç¤¾åŒºèµ„æº

- [Hunyuan-7B åœ¨ CNB ä¸­å¿«é€Ÿå¼€å§‹](https://cnb.cool/tencent/hunyuan/examples/Hunyuan-7B-Instruct-quick-start)

## è”ç³»æˆ‘ä»¬
å¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œæ¬¢è¿è”ç³»æˆ‘ä»¬è…¾è®¯æ··å…ƒLLMå›¢é˜Ÿã€‚ä½ å¯ä»¥é€šè¿‡é‚®ä»¶ï¼ˆhunyuan_opensource@tencent.comï¼‰è”ç³»æˆ‘ä»¬ã€‚
